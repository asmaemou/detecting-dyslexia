{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **OmniVec-like**\n",
    "  More advanced model architecture that resembles the functionality and complexity of OmniVec. Since OmniVec is not a standard model, we'll implement a complex Convolutional Neural Network (CNN) with multiple layers, similar to how OmniVec would be structured.\n",
    "\n",
    "\n",
    "I'll provide a detailed model architecture using TensorFlow/Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, concatenate\n",
    "\n",
    "# Set the correct paths for train_dir and test_dir\n",
    "train_dir = '/Users/asmae/Documents/GitHub/detecting-dyslexia/Gambo/Train'\n",
    "test_dir = '/Users/asmae/Documents/GitHub/detecting-dyslexia/Gambo/Test'\n",
    "\n",
    "# Prepare data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')\n",
    "\n",
    "# Define the OmniVec model\n",
    "input_layer = Input(shape=(150, 150, 3))\n",
    "\n",
    "# First block\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Second block\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Third block\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Fourth block\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Global Average Pooling\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Fully connected layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // 32,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // 32,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(test_generator)\n",
    "print(f'Test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the correct paths for train_dir and test_dir\n",
    "train_dir = '/Users/asmae/Documents/GitHub/detecting-dyslexia/Gambo/Train'\n",
    "test_dir = '/Users/asmae/Documents/GitHub/detecting-dyslexia/Gambo/Test'\n",
    "\n",
    "# Prepare data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')\n",
    "\n",
    "# Define the OmniVec model\n",
    "input_layer = Input(shape=(150, 150, 3))\n",
    "\n",
    "# First block\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Second block\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Third block\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Fourth block\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Global Average Pooling\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Fully connected layers\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // 32,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // 32,\n",
    "    epochs=3,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(test_generator)\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "\n",
    "# Evaluate the model and print confusion matrix and classification report\n",
    "test_steps = test_generator.samples // test_generator.batch_size\n",
    "predictions = model.predict(test_generator, steps=test_steps+1)\n",
    "predicted_classes = (predictions > 0.5).astype(\"int32\")\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(true_classes, predicted_classes, target_names=test_generator.class_indices.keys())\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(test_generator.class_indices))\n",
    "plt.xticks(tick_marks, test_generator.class_indices.keys(), rotation=45)\n",
    "plt.yticks(tick_marks, test_generator.class_indices.keys())\n",
    "\n",
    "fmt = 'd'\n",
    "thresh = conf_matrix.max() / 2.\n",
    "for i, j in np.ndindex(conf_matrix.shape):\n",
    "    plt.text(j, i, format(conf_matrix[i, j], fmt),\n",
    "             ha=\"center\", va=\"center\",\n",
    "             color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_image(model, img_path):\n",
    "    img = load_img(img_path, target_size=(150, 150), color_mode='rgb')\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Create batch dimension\n",
    "    img_array /= 255.0  # Normalize\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "    if prediction > 0.5:\n",
    "        return 'Dyslexia Detected'\n",
    "    else:\n",
    "        return 'No Dyslexia Detected'\n",
    "\n",
    "# Example usage\n",
    "print(predict_image(model, '/Users/asmae/Documents/GitHub/detecting-dyslexia/Gambo/Test/Normal/A-42.png'))\n",
    "\n",
    "# Count images in each directory\n",
    "def count_images(directory):\n",
    "    class_counts = {}\n",
    "    for class_dir in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            class_counts[class_dir] = len(os.listdir(class_path))\n",
    "    return class_counts\n",
    "\n",
    "# Count images in train and test directories\n",
    "train_class_counts = count_images(train_dir)\n",
    "test_class_counts = count_images(test_dir)\n",
    "\n",
    "print(\"Images per class in train:\")\n",
    "for class_name, count in train_class_counts.items():\n",
    "    print(f\"  {class_name}: {count}\")\n",
    "\n",
    "print(\"Images per class in test:\")\n",
    "for class_name, count in test_class_counts.items():\n",
    "    print(f\"  {class_name}: {count}\")\n",
    "\n",
    "# Perform data augmentation if classes are imbalanced\n",
    "min_class_count = min(train_class_counts.values())\n",
    "\n",
    "# Data augmentation and normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "def augment_class(directory, class_name, target_count):\n",
    "    class_path = os.path.join(directory, class_name)\n",
    "    current_count = len(os.listdir(class_path))\n",
    "    if current_count < target_count:\n",
    "        datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "        batch_size = 32\n",
    "        target_batches = (target_count - current_count) // batch_size + 1\n",
    "        generator = datagen.flow_from_directory(\n",
    "            directory,\n",
    "            classes=[class_name],\n",
    "            target_size=(150, 150),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='binary',\n",
    "            save_to_dir=class_path,\n",
    "            save_prefix='aug',\n",
    "            save_format='jpeg'\n",
    "        )\n",
    "        for i in range(target_batches):\n",
    "            generator.next()\n",
    "            print(f\"Augmented batch {i+1} for class {class_name}\")\n",
    "\n",
    "# Augment data for classes in train if needed\n",
    "for class_name, count in train_class_counts.items():\n",
    "    augment_class(train_dir, class_name, min_class_count)\n",
    "\n",
    "print(\"Data augmentation completed if needed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
